{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtu-QVrhlK47"
      },
      "source": [
        "# **Financial Impact Analysis Using RoBERTa**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY37iGU-kqyX"
      },
      "source": [
        "## **Install necessary packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JZrYwzeLzlp",
        "outputId": "5187337d-d27c-4f66-9db2-9aec0c9b55c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.10/dist-packages (0.19.1)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.22.2)\n",
            "Requirement already satisfied: tensorboard==2.11 in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: trl in /usr/local/lib/python3.10/dist-packages (0.8.6)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (1.62.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (3.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (1.25.2)\n",
            "Requirement already satisfied: protobuf<4,>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (67.7.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (3.0.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.11) (0.43.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.11.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from trl) (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from trl) (0.29.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from trl) (2.19.0)\n",
            "Requirement already satisfied: tyro>=0.5.11 in /usr/local/lib/python3.10/dist-packages (from trl) (0.8.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.11) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.11) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard==2.11) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.11) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.11) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.11) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.11) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard==2.11) (2024.2.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.4.0->trl) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->trl) (12.4.127)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (13.7.1)\n",
            "Requirement already satisfied: shtab>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.11) (2.1.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->trl) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->trl) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->trl) (4.0.3)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard==2.11) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard==2.11) (3.2.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->trl) (2024.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers tokenizers huggingface_hub tensorboard==2.11 trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9-vM4ThewD6B"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
            "CUDA SETUP: Loading binary /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
            "dlopen(/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    RobertaTokenizerFast,\n",
        "    RobertaConfig,\n",
        "    RobertaForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from pathlib import Path\n",
        "from huggingface_hub import PyTorchModelHubMixin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxqULnnWxOHX"
      },
      "source": [
        "## **Preprocessings**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dzf1j0RVlccy",
        "outputId": "7b88311c-e744-411e-ecb1-4ed281a12bbc"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "dataset = pd.read_csv('./forex_factory_dataset.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEcvP5pwHKrG"
      },
      "source": [
        "### Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gcBvOFv6HLJk"
      },
      "outputs": [],
      "source": [
        "# Define function to remove non-numeric characters\n",
        "def remove_non_numeric(text):\n",
        "    pattern = r'[^0-9.]'\n",
        "    result = re.sub(pattern, '', text)\n",
        "    return result\n",
        "\n",
        "def impact_coding(impact):\n",
        "  if impact == \"Low Impact Expected\":\n",
        "    return 0\n",
        "  elif impact == \"Medium Impact Expected\":\n",
        "    return 1\n",
        "  else:\n",
        "    return 2\n",
        "\n",
        "dataset['impact'] = dataset['impact'].map(lambda row: impact_coding(row))\n",
        "dataset.dropna(subset=['actual_value'], inplace=True)\n",
        "dataset['forecast_value'].fillna(dataset['previous_value'], inplace=True)\n",
        "dataset.dropna(inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 762
        },
        "id": "N_th-X6hIlmz",
        "outputId": "eedef46a-92bb-4d91-ef2e-6455c8aa7bee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>currency</th>\n",
              "      <th>event</th>\n",
              "      <th>impact</th>\n",
              "      <th>previous_value</th>\n",
              "      <th>forecast_value</th>\n",
              "      <th>actual_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>12</td>\n",
              "      <td>JPY</td>\n",
              "      <td>Final Manufacturing PMI</td>\n",
              "      <td>0</td>\n",
              "      <td>49.7</td>\n",
              "      <td>49.7</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>13</td>\n",
              "      <td>CNY</td>\n",
              "      <td>Caixin Manufacturing PMI</td>\n",
              "      <td>0</td>\n",
              "      <td>54.9</td>\n",
              "      <td>54.7</td>\n",
              "      <td>53.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>14</td>\n",
              "      <td>AUD</td>\n",
              "      <td>Commodity Prices y/y</td>\n",
              "      <td>0</td>\n",
              "      <td>2.5%</td>\n",
              "      <td>2.5%</td>\n",
              "      <td>11.7%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>15</td>\n",
              "      <td>EUR</td>\n",
              "      <td>Spanish Manufacturing PMI</td>\n",
              "      <td>0</td>\n",
              "      <td>49.8</td>\n",
              "      <td>52.6</td>\n",
              "      <td>51.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>16</td>\n",
              "      <td>CHF</td>\n",
              "      <td>Manufacturing PMI</td>\n",
              "      <td>0</td>\n",
              "      <td>55.2</td>\n",
              "      <td>54.4</td>\n",
              "      <td>58.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15178</th>\n",
              "      <td>15178</td>\n",
              "      <td>GBP</td>\n",
              "      <td>30-y Bond Auction</td>\n",
              "      <td>0</td>\n",
              "      <td>2.53|2.2</td>\n",
              "      <td>2.53|2.2</td>\n",
              "      <td>2.36|2.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15179</th>\n",
              "      <td>15179</td>\n",
              "      <td>USD</td>\n",
              "      <td>NFIB Small Business Index</td>\n",
              "      <td>0</td>\n",
              "      <td>89.5</td>\n",
              "      <td>89.5</td>\n",
              "      <td>89.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15180</th>\n",
              "      <td>15180</td>\n",
              "      <td>USD</td>\n",
              "      <td>Prelim Nonfarm Productivity q/q</td>\n",
              "      <td>0</td>\n",
              "      <td>-7.3%</td>\n",
              "      <td>-4.6%</td>\n",
              "      <td>-4.6%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15181</th>\n",
              "      <td>15181</td>\n",
              "      <td>USD</td>\n",
              "      <td>Prelim Unit Labor Costs q/q</td>\n",
              "      <td>0</td>\n",
              "      <td>12.6%</td>\n",
              "      <td>9.4%</td>\n",
              "      <td>10.8%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15182</th>\n",
              "      <td>15182</td>\n",
              "      <td>USD</td>\n",
              "      <td>RCM/TIPP Economic Optimism</td>\n",
              "      <td>0</td>\n",
              "      <td>38.5</td>\n",
              "      <td>40.2</td>\n",
              "      <td>38.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>12109 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0 currency                            event  impact  \\\n",
              "12             12      JPY          Final Manufacturing PMI       0   \n",
              "13             13      CNY         Caixin Manufacturing PMI       0   \n",
              "14             14      AUD             Commodity Prices y/y       0   \n",
              "15             15      EUR        Spanish Manufacturing PMI       0   \n",
              "16             16      CHF                Manufacturing PMI       0   \n",
              "...           ...      ...                              ...     ...   \n",
              "15178       15178      GBP                30-y Bond Auction       0   \n",
              "15179       15179      USD        NFIB Small Business Index       0   \n",
              "15180       15180      USD  Prelim Nonfarm Productivity q/q       0   \n",
              "15181       15181      USD      Prelim Unit Labor Costs q/q       0   \n",
              "15182       15182      USD       RCM/TIPP Economic Optimism       0   \n",
              "\n",
              "      previous_value forecast_value actual_value  \n",
              "12              49.7           49.7         50.0  \n",
              "13              54.9           54.7         53.0  \n",
              "14              2.5%           2.5%        11.7%  \n",
              "15              49.8           52.6         51.0  \n",
              "16              55.2           54.4         58.0  \n",
              "...              ...            ...          ...  \n",
              "15178       2.53|2.2       2.53|2.2     2.36|2.5  \n",
              "15179           89.5           89.5         89.9  \n",
              "15180          -7.3%          -4.6%        -4.6%  \n",
              "15181          12.6%           9.4%        10.8%  \n",
              "15182           38.5           40.2         38.1  \n",
              "\n",
              "[12109 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "RPnA-6AwSC9i"
      },
      "outputs": [],
      "source": [
        "condition_mask = dataset['previous_value'].str.contains(\"|\",regex=False)\n",
        "dataset = dataset[~condition_mask]\n",
        "dataset = dataset.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 779
        },
        "id": "w4Wd1vHrSIj2",
        "outputId": "ab663c0f-425a-40f3-8edd-309747260a16"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>currency</th>\n",
              "      <th>event</th>\n",
              "      <th>impact</th>\n",
              "      <th>previous_value</th>\n",
              "      <th>forecast_value</th>\n",
              "      <th>actual_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12</td>\n",
              "      <td>JPY</td>\n",
              "      <td>Final Manufacturing PMI</td>\n",
              "      <td>0</td>\n",
              "      <td>49.7</td>\n",
              "      <td>49.7</td>\n",
              "      <td>50.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13</td>\n",
              "      <td>CNY</td>\n",
              "      <td>Caixin Manufacturing PMI</td>\n",
              "      <td>0</td>\n",
              "      <td>54.9</td>\n",
              "      <td>54.7</td>\n",
              "      <td>53.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>14</td>\n",
              "      <td>AUD</td>\n",
              "      <td>Commodity Prices y/y</td>\n",
              "      <td>0</td>\n",
              "      <td>2.5%</td>\n",
              "      <td>2.5%</td>\n",
              "      <td>11.7%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>15</td>\n",
              "      <td>EUR</td>\n",
              "      <td>Spanish Manufacturing PMI</td>\n",
              "      <td>0</td>\n",
              "      <td>49.8</td>\n",
              "      <td>52.6</td>\n",
              "      <td>51.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>16</td>\n",
              "      <td>CHF</td>\n",
              "      <td>Manufacturing PMI</td>\n",
              "      <td>0</td>\n",
              "      <td>55.2</td>\n",
              "      <td>54.4</td>\n",
              "      <td>58.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11696</th>\n",
              "      <td>15177</td>\n",
              "      <td>JPY</td>\n",
              "      <td>Prelim Machine Tool Orders y/y</td>\n",
              "      <td>0</td>\n",
              "      <td>17.1%</td>\n",
              "      <td>17.1%</td>\n",
              "      <td>5.5%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11697</th>\n",
              "      <td>15179</td>\n",
              "      <td>USD</td>\n",
              "      <td>NFIB Small Business Index</td>\n",
              "      <td>0</td>\n",
              "      <td>89.5</td>\n",
              "      <td>89.5</td>\n",
              "      <td>89.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11698</th>\n",
              "      <td>15180</td>\n",
              "      <td>USD</td>\n",
              "      <td>Prelim Nonfarm Productivity q/q</td>\n",
              "      <td>0</td>\n",
              "      <td>-7.3%</td>\n",
              "      <td>-4.6%</td>\n",
              "      <td>-4.6%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11699</th>\n",
              "      <td>15181</td>\n",
              "      <td>USD</td>\n",
              "      <td>Prelim Unit Labor Costs q/q</td>\n",
              "      <td>0</td>\n",
              "      <td>12.6%</td>\n",
              "      <td>9.4%</td>\n",
              "      <td>10.8%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11700</th>\n",
              "      <td>15182</td>\n",
              "      <td>USD</td>\n",
              "      <td>RCM/TIPP Economic Optimism</td>\n",
              "      <td>0</td>\n",
              "      <td>38.5</td>\n",
              "      <td>40.2</td>\n",
              "      <td>38.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>11701 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0 currency                            event  impact  \\\n",
              "0              12      JPY          Final Manufacturing PMI       0   \n",
              "1              13      CNY         Caixin Manufacturing PMI       0   \n",
              "2              14      AUD             Commodity Prices y/y       0   \n",
              "3              15      EUR        Spanish Manufacturing PMI       0   \n",
              "4              16      CHF                Manufacturing PMI       0   \n",
              "...           ...      ...                              ...     ...   \n",
              "11696       15177      JPY   Prelim Machine Tool Orders y/y       0   \n",
              "11697       15179      USD        NFIB Small Business Index       0   \n",
              "11698       15180      USD  Prelim Nonfarm Productivity q/q       0   \n",
              "11699       15181      USD      Prelim Unit Labor Costs q/q       0   \n",
              "11700       15182      USD       RCM/TIPP Economic Optimism       0   \n",
              "\n",
              "      previous_value forecast_value actual_value  \n",
              "0               49.7           49.7         50.0  \n",
              "1               54.9           54.7         53.0  \n",
              "2               2.5%           2.5%        11.7%  \n",
              "3               49.8           52.6         51.0  \n",
              "4               55.2           54.4         58.0  \n",
              "...              ...            ...          ...  \n",
              "11696          17.1%          17.1%         5.5%  \n",
              "11697           89.5           89.5         89.9  \n",
              "11698          -7.3%          -4.6%        -4.6%  \n",
              "11699          12.6%           9.4%        10.8%  \n",
              "11700           38.5           40.2         38.1  \n",
              "\n",
              "[11701 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gZjqAT3hBVrl"
      },
      "outputs": [],
      "source": [
        "dataset['previous_value'] = dataset['previous_value'].apply(remove_non_numeric)\n",
        "dataset['forecast_value'] = dataset['forecast_value'].apply(remove_non_numeric)\n",
        "dataset['actual_value'] = dataset['actual_value'].apply(remove_non_numeric)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "38ynKmUfG_3T"
      },
      "outputs": [],
      "source": [
        "X = dataset.drop(columns=[\"impact\"])\n",
        "y = dataset[\"impact\"]\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)\n",
        "\n",
        "# Split the data into test and validation sets\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.5,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Oh60T6AYLMUO"
      },
      "outputs": [],
      "source": [
        "X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
        "X_test, y_test = X_test.reset_index(drop=True), y_test.reset_index(drop=True)\n",
        "X_val, y_val = X_val.reset_index(drop=True), y_val.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z9SFrc4eedZ",
        "outputId": "c213c4db-6bae-4d05-d3c9-0ad98112fde5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       0\n",
              "1       0\n",
              "2       0\n",
              "3       2\n",
              "4       1\n",
              "       ..\n",
              "9355    0\n",
              "9356    0\n",
              "9357    0\n",
              "9358    0\n",
              "9359    0\n",
              "Name: impact, Length: 9360, dtype: int64"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaVN4bjWa5G9"
      },
      "source": [
        "## **Data Loader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tJGhvGe3a-x7"
      },
      "outputs": [],
      "source": [
        "# Define a custom dataset\n",
        "class FinancialNewsData(Dataset):\n",
        "    def __init__(self, X, y, tokenizer, max_length):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.X.iloc[idx]['event'])\n",
        "        target = self.y.iloc[idx]\n",
        "        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': target\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkF1pdNlbhBu"
      },
      "source": [
        "## **Model Definition**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UeUQ2fzWbkdC"
      },
      "outputs": [],
      "source": [
        "# Define your custom model architecture\n",
        "\n",
        "class FinancialNewsImpactPredictor(nn.Module, PyTorchModelHubMixin):\n",
        "    def __init__(self, num_classes=3):\n",
        "      super(FinancialNewsImpactPredictor,self).__init__()\n",
        "      config = RobertaConfig()\n",
        "      config.num_labels = num_classes\n",
        "      self.roberta = RobertaForSequenceClassification(config=config)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "      output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "      return torch.softmax(output.logits, dim=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeUi7IYIaXN9"
      },
      "source": [
        "## **Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CXPIJEdQ1MO"
      },
      "source": [
        "### Set tokenizer and dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "fXZJH76aQ6xp"
      },
      "outputs": [],
      "source": [
        "# Select hyperparameters\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 1e-5\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "\n",
        "train_dataset = FinancialNewsData(X_train, y_train, tokenizer, max_length=128)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_dataset = FinancialNewsData(X_val, y_val, tokenizer, max_length=128)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTlMXvwJyKHj"
      },
      "source": [
        "### Set model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9NMJ6vfeyMIE"
      },
      "outputs": [],
      "source": [
        "model = FinancialNewsImpactPredictor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b3xKIrByd2F"
      },
      "source": [
        "### Set trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1Cjeh_IRygxo"
      },
      "outputs": [],
      "source": [
        "# Select criterion and optimizers\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters(),lr=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWIWI-iqJZs-"
      },
      "source": [
        "### **Train**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sGOOiHik9A4"
      },
      "source": [
        "### Train and evaluation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "-L_4W3R8kz4x"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device, pbar):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Update the tqdm progress bar\n",
        "        pbar.update(1)\n",
        "        pbar.set_postfix({'loss': loss.item()})\n",
        "\n",
        "    return running_loss / len(train_loader)\n",
        "\n",
        "def evaluate(model, val_loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    return total_loss / len(val_loader), total_correct / total_samples\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTUoREIKlFKH"
      },
      "source": [
        "### Traning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "aBGUiEHKJZYz",
        "outputId": "1ab7a82f-6143-454f-f439-4709b169eb49"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/3: 100%|██████████| 585/585 [15:36<00:00,  1.60s/batch, loss=0.989]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3, Train Loss: 0.7865, Val Loss: 0.7793, Val Acc: 0.7771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/3: 100%|██████████| 585/585 [16:28<00:00,  1.69s/batch, loss=0.989]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/3, Train Loss: 0.7854, Val Loss: 0.7756, Val Acc: 0.7771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/3: 100%|██████████| 585/585 [15:16<00:00,  1.57s/batch, loss=0.739]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/3, Train Loss: 0.7853, Val Loss: 0.7719, Val Acc: 0.7771\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "  with tqdm(total=len(train_loader), desc=f'Epoch {epoch+1}/{NUM_EPOCHS}', unit='batch') as pbar:\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device, pbar)\n",
        "\n",
        "  val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "  print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfBKbKy1WPW3"
      },
      "source": [
        "## **Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "9DA60M6HWOGB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss: 0.7719000376559593 Accuracy: 0.7771135781383433\n"
          ]
        }
      ],
      "source": [
        "test_dataset = FinancialNewsData(X_test, y_test, tokenizer, max_length=128)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loss, test_acc = evaluate(model, val_loader, criterion, device)\n",
        "print(f\"Loss: {test_loss} Accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1yOVlnZrJ5fw"
      },
      "source": [
        "## **Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "f97P9h0CJ5C2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "models/roberta-financial-news-impact-analysis.bin\n"
          ]
        }
      ],
      "source": [
        "filename = Path('models')\n",
        "filename.mkdir(parents=True,exist_ok=True)\n",
        "\n",
        "model_name='roberta-financial-news-impact-analysis.bin' # model name\n",
        "\n",
        "\n",
        "# saving path\n",
        "\n",
        "saving_path = filename/model_name\n",
        "print(saving_path)\n",
        "torch.save(obj=model.state_dict(),f=saving_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Make Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "label_mapping = {\n",
        "    0: \"Low\",\n",
        "    1: \"Medium\",\n",
        "    2: \"High\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From local saved"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Impact: Low\n"
          ]
        }
      ],
      "source": [
        "model_path = \"./models/model_v1/pytorch_model.bin\"\n",
        "model = FinancialNewsImpactPredictor()\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "input_text = \"German Buba President Nagel Speaks\"\n",
        "encoding = tokenizer(input_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "input_ids =  encoding['input_ids'].flatten()\n",
        "attention_mask = encoding['attention_mask'].flatten()\n",
        "input_ids = input_ids.unsqueeze(0)\n",
        "attention_mask = attention_mask.unsqueeze(0)\n",
        "\n",
        "output = model(input_ids,attention_mask)\n",
        "predicted_class_index = torch.argmax(output)\n",
        "predicted_label = label_mapping[predicted_class_index.item()]\n",
        "print(\"Predicted Impact:\", predicted_label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### From HuggingFace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at nusretkizilaslan/roberta-financial-news-impact-analysis and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.LayerNorm.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.embeddings.position_embeddings.weight', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.11.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.6.output.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.encoder.layer.9.output.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted Impact: Low\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import RobertaTokenizerFast\n",
        "import torch\n",
        "\n",
        "\n",
        "MODEL = \"nusret35/roberta-financial-news-impact-analysis\"\n",
        "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "\n",
        "input_text = \"German Buba President Nagel Speaks\"\n",
        "encoding = tokenizer(input_text, padding='max_length', truncation=True, max_length=128, return_tensors='pt')\n",
        "input_ids =  encoding['input_ids'].flatten()\n",
        "attention_mask = encoding['attention_mask'].flatten()\n",
        "input_ids = input_ids.unsqueeze(0)\n",
        "attention_mask = attention_mask.unsqueeze(0)\n",
        "\n",
        "output = model(input_ids,attention_mask)\n",
        "predicted_class_index = torch.argmax(output.logits)\n",
        "predicted_label = label_mapping[predicted_class_index.item()]\n",
        "print(\"Predicted Impact:\", predicted_label)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
